{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnOgaPA41fkr"
      },
      "source": [
        "# COMP34812 Natural Language Understanding Courseworklow key lemming an stemming\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSUwder1fkt"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqR9evYQ1fku",
        "outputId": "f35fd907-8027-4e57-c0c3-42bca35f21fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: nltk in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: tokenizers in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from tokenizers) (0.29.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zaccu\\onedrive\\documents\\github\\comp34812-nlu-nli\\.venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk numpy matplotlib scikit-learn sentencepiece tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UChWsR7O1fku",
        "outputId": "588c9b21-3c6e-41cc-a728-3ea9245dd8de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\zaccu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\zaccu\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from tokenizers import ByteLevelBPETokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5Qier3D1iSF"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('glove_embeddings'):\n",
        "  !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "  !unzip glove.6B.zip -d glove_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_DqwdkY1fkv"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhHy7Sby1fkw",
        "outputId": "147a3ce0-7d39-4d7c-c395-9bb5f111886e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>By starting at the soft underbelly, the 16,000...</td>\n",
              "      <td>General Nelson A. Miles had 30,000 troops in h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The class had broken into a light sweat, but w...</td>\n",
              "      <td>The class grew more tense as time went on.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Samson had his famous haircut here, but he wou...</td>\n",
              "      <td>It was unknown where exactly within the town S...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A man with a black shirt holds a baby while a ...</td>\n",
              "      <td>A darkly dressed man passes a crying baby to a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know that many of you are interested in addr...</td>\n",
              "      <td>The problems must be addressed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0  By starting at the soft underbelly, the 16,000...   \n",
              "1  The class had broken into a light sweat, but w...   \n",
              "2  Samson had his famous haircut here, but he wou...   \n",
              "3  A man with a black shirt holds a baby while a ...   \n",
              "4  I know that many of you are interested in addr...   \n",
              "\n",
              "                                          hypothesis  label  \n",
              "0  General Nelson A. Miles had 30,000 troops in h...      0  \n",
              "1         The class grew more tense as time went on.      1  \n",
              "2  It was unknown where exactly within the town S...      1  \n",
              "3  A darkly dressed man passes a crying baby to a...      0  \n",
              "4                    The problems must be addressed       1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set = pd.read_csv('dev.csv')\n",
        "dev_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQlmvS1I1fkw",
        "outputId": "2a25e645-aa64-4689-c8c3-0bea90d89255"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>yeah i don't know cut California in half or so...</td>\n",
              "      <td>Yeah. I'm not sure how to make that fit. Maybe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actual names will not be used</td>\n",
              "      <td>For the sake of privacy, actual names are not ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The film was directed by Randall Wallace.</td>\n",
              "      <td>The film was directed by Randall Wallace and s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"How d'you know he'll sign me on?\"Anse studie...</td>\n",
              "      <td>Anse looked at himself in a cracked mirror.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In the light of the candles his cheeks looked ...</td>\n",
              "      <td>Drew regarded his best friend and noted that i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0  yeah i don't know cut California in half or so...   \n",
              "1                      actual names will not be used   \n",
              "2          The film was directed by Randall Wallace.   \n",
              "3   \"How d'you know he'll sign me on?\"Anse studie...   \n",
              "4  In the light of the candles his cheeks looked ...   \n",
              "\n",
              "                                          hypothesis  label  \n",
              "0  Yeah. I'm not sure how to make that fit. Maybe...      1  \n",
              "1  For the sake of privacy, actual names are not ...      1  \n",
              "2  The film was directed by Randall Wallace and s...      1  \n",
              "3       Anse looked at himself in a cracked mirror.       1  \n",
              "4  Drew regarded his best friend and noted that i...      1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set = pd.read_csv('train.csv')\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExYfX91t1fkx"
      },
      "outputs": [],
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    processed = []\n",
        "    for word in text:\n",
        "        if word in stop_words:\n",
        "            continue\n",
        "\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "\n",
        "        word = word.strip()\n",
        "\n",
        "        if len(word) < 2:\n",
        "            continue\n",
        "\n",
        "        processed.append(word)\n",
        "\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqAFgu8l1fky"
      },
      "outputs": [],
      "source": [
        "dev_set['premise_tokens'] = dev_set['premise'].apply(clean_text)\n",
        "dev_set['hypothesis_tokens'] = dev_set['hypothesis'].apply(clean_text)\n",
        "\n",
        "train_set['premise_tokens'] = train_set['premise'].apply(clean_text)\n",
        "train_set['hypothesis_tokens'] = train_set['hypothesis'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRmvOcx11fkz",
        "outputId": "eb6a838f-282e-4e8f-dfb5-ee330aeb27e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "      <th>premise_tokens</th>\n",
              "      <th>hypothesis_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>By starting at the soft underbelly, the 16,000...</td>\n",
              "      <td>General Nelson A. Miles had 30,000 troops in h...</td>\n",
              "      <td>0</td>\n",
              "      <td>[starting, soft, underbelly, 16, 000, troop, g...</td>\n",
              "      <td>[general, nelson, mile, 30, 000, troop, attack]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The class had broken into a light sweat, but w...</td>\n",
              "      <td>The class grew more tense as time went on.</td>\n",
              "      <td>1</td>\n",
              "      <td>[class, broken, light, sweat, gasping, air]</td>\n",
              "      <td>[class, grew, tense, time, went]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Samson had his famous haircut here, but he wou...</td>\n",
              "      <td>It was unknown where exactly within the town S...</td>\n",
              "      <td>1</td>\n",
              "      <td>[samson, famous, haircut, would, find, hard, r...</td>\n",
              "      <td>[unknown, exactly, within, town, samson, recei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A man with a black shirt holds a baby while a ...</td>\n",
              "      <td>A darkly dressed man passes a crying baby to a...</td>\n",
              "      <td>0</td>\n",
              "      <td>[man, black, shirt, hold, baby, blue, shirted,...</td>\n",
              "      <td>[darkly, dressed, man, pass, cry, baby, man, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know that many of you are interested in addr...</td>\n",
              "      <td>The problems must be addressed</td>\n",
              "      <td>1</td>\n",
              "      <td>[know, many, interested, addressing, issue, le...</td>\n",
              "      <td>[problem, must, addressed]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0  By starting at the soft underbelly, the 16,000...   \n",
              "1  The class had broken into a light sweat, but w...   \n",
              "2  Samson had his famous haircut here, but he wou...   \n",
              "3  A man with a black shirt holds a baby while a ...   \n",
              "4  I know that many of you are interested in addr...   \n",
              "\n",
              "                                          hypothesis  label  \\\n",
              "0  General Nelson A. Miles had 30,000 troops in h...      0   \n",
              "1         The class grew more tense as time went on.      1   \n",
              "2  It was unknown where exactly within the town S...      1   \n",
              "3  A darkly dressed man passes a crying baby to a...      0   \n",
              "4                    The problems must be addressed       1   \n",
              "\n",
              "                                      premise_tokens  \\\n",
              "0  [starting, soft, underbelly, 16, 000, troop, g...   \n",
              "1        [class, broken, light, sweat, gasping, air]   \n",
              "2  [samson, famous, haircut, would, find, hard, r...   \n",
              "3  [man, black, shirt, hold, baby, blue, shirted,...   \n",
              "4  [know, many, interested, addressing, issue, le...   \n",
              "\n",
              "                                   hypothesis_tokens  \n",
              "0    [general, nelson, mile, 30, 000, troop, attack]  \n",
              "1                   [class, grew, tense, time, went]  \n",
              "2  [unknown, exactly, within, town, samson, recei...  \n",
              "3  [darkly, dressed, man, pass, cry, baby, man, l...  \n",
              "4                         [problem, must, addressed]  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd8StseU1fkz",
        "outputId": "cbe361f2-5de1-43fa-ccc9-9639312aefca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "      <th>premise_tokens</th>\n",
              "      <th>hypothesis_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>yeah i don't know cut California in half or so...</td>\n",
              "      <td>Yeah. I'm not sure how to make that fit. Maybe...</td>\n",
              "      <td>1</td>\n",
              "      <td>[yeah, know, cut, california, half, something]</td>\n",
              "      <td>[yeah, sure, make, fit, maybe, could, cut, cal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actual names will not be used</td>\n",
              "      <td>For the sake of privacy, actual names are not ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[actual, name, used]</td>\n",
              "      <td>[sake, privacy, actual, name, used]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The film was directed by Randall Wallace.</td>\n",
              "      <td>The film was directed by Randall Wallace and s...</td>\n",
              "      <td>1</td>\n",
              "      <td>[film, directed, randall, wallace]</td>\n",
              "      <td>[film, directed, randall, wallace, star, mel, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"How d'you know he'll sign me on?\"Anse studie...</td>\n",
              "      <td>Anse looked at himself in a cracked mirror.</td>\n",
              "      <td>1</td>\n",
              "      <td>[know, sign, anse, studied, unkempt, clean, re...</td>\n",
              "      <td>[anse, looked, cracked, mirror]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In the light of the candles his cheeks looked ...</td>\n",
              "      <td>Drew regarded his best friend and noted that i...</td>\n",
              "      <td>1</td>\n",
              "      <td>[light, candle, cheek, looked, even, hollow, t...</td>\n",
              "      <td>[drew, regarded, best, friend, noted, light, l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0  yeah i don't know cut California in half or so...   \n",
              "1                      actual names will not be used   \n",
              "2          The film was directed by Randall Wallace.   \n",
              "3   \"How d'you know he'll sign me on?\"Anse studie...   \n",
              "4  In the light of the candles his cheeks looked ...   \n",
              "\n",
              "                                          hypothesis  label  \\\n",
              "0  Yeah. I'm not sure how to make that fit. Maybe...      1   \n",
              "1  For the sake of privacy, actual names are not ...      1   \n",
              "2  The film was directed by Randall Wallace and s...      1   \n",
              "3       Anse looked at himself in a cracked mirror.       1   \n",
              "4  Drew regarded his best friend and noted that i...      1   \n",
              "\n",
              "                                      premise_tokens  \\\n",
              "0     [yeah, know, cut, california, half, something]   \n",
              "1                               [actual, name, used]   \n",
              "2                 [film, directed, randall, wallace]   \n",
              "3  [know, sign, anse, studied, unkempt, clean, re...   \n",
              "4  [light, candle, cheek, looked, even, hollow, t...   \n",
              "\n",
              "                                   hypothesis_tokens  \n",
              "0  [yeah, sure, make, fit, maybe, could, cut, cal...  \n",
              "1                [sake, privacy, actual, name, used]  \n",
              "2  [film, directed, randall, wallace, star, mel, ...  \n",
              "3                    [anse, looked, cracked, mirror]  \n",
              "4  [drew, regarded, best, friend, noted, light, l...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9JrXj2d1fk1"
      },
      "source": [
        "Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmG2nSeG1fk1",
        "outputId": "66a408f7-fad0-46b6-de17-657558e0411b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "      <th>premise_tokens</th>\n",
              "      <th>hypothesis_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2166</th>\n",
              "      <td>No, he waits until he has had a violent quarre...</td>\n",
              "      <td>He was clear and free from suspicion.</td>\n",
              "      <td>0</td>\n",
              "      <td>[wait, violent, quarrel, whole, household, cog...</td>\n",
              "      <td>[clear, free, suspicion]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10333</th>\n",
              "      <td>oh yeah but that's that's a good way to make a...</td>\n",
              "      <td>A marriage can work when you do dishonest thin...</td>\n",
              "      <td>0</td>\n",
              "      <td>[oh, yeah, good, way, make, big, problem, marr...</td>\n",
              "      <td>[marriage, work, dishonest, thing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10835</th>\n",
              "      <td>Land that is acquired for or in connection wit...</td>\n",
              "      <td>Land that is acquired has no connection with i...</td>\n",
              "      <td>0</td>\n",
              "      <td>[land, acquired, connection, item, general, pp...</td>\n",
              "      <td>[land, acquired, connection, item, general, pp]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4688</th>\n",
              "      <td>and my wife is from Plains if you know where P...</td>\n",
              "      <td>My wife went to school in Plains.</td>\n",
              "      <td>1</td>\n",
              "      <td>[wife, plain, know, plain]</td>\n",
              "      <td>[wife, went, school, plain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17561</th>\n",
              "      <td>Since 1980 the building has housed the Museum ...</td>\n",
              "      <td>The building has never been able to house any ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[since, 1980, building, housed, museum, ethnog...</td>\n",
              "      <td>[building, never, able, house, museum]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 premise  \\\n",
              "2166   No, he waits until he has had a violent quarre...   \n",
              "10333  oh yeah but that's that's a good way to make a...   \n",
              "10835  Land that is acquired for or in connection wit...   \n",
              "4688   and my wife is from Plains if you know where P...   \n",
              "17561  Since 1980 the building has housed the Museum ...   \n",
              "\n",
              "                                              hypothesis  label  \\\n",
              "2166              He was clear and free from suspicion.       0   \n",
              "10333  A marriage can work when you do dishonest thin...      0   \n",
              "10835  Land that is acquired has no connection with i...      0   \n",
              "4688                   My wife went to school in Plains.      1   \n",
              "17561  The building has never been able to house any ...      0   \n",
              "\n",
              "                                          premise_tokens  \\\n",
              "2166   [wait, violent, quarrel, whole, household, cog...   \n",
              "10333  [oh, yeah, good, way, make, big, problem, marr...   \n",
              "10835  [land, acquired, connection, item, general, pp...   \n",
              "4688                          [wife, plain, know, plain]   \n",
              "17561  [since, 1980, building, housed, museum, ethnog...   \n",
              "\n",
              "                                     hypothesis_tokens  \n",
              "2166                          [clear, free, suspicion]  \n",
              "10333               [marriage, work, dishonest, thing]  \n",
              "10835  [land, acquired, connection, item, general, pp]  \n",
              "4688                       [wife, went, school, plain]  \n",
              "17561           [building, never, able, house, museum]  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Randomly select 90% of the rows for the training set\n",
        "train_df = train_set.sample(frac=0.9, random_state=42)\n",
        "# The rest of the rows will form the test set\n",
        "test_df = train_set.drop(train_df.index)\n",
        "train_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODT1qHdIhOZv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk3d02wchOZv"
      },
      "source": [
        "# BPE tokinizer (might use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qseCu8ldhOZv"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "\n",
        "if not os.path.exists(\"byte_bpe\"):\n",
        "    os.makedirs(\"byte_bpe\")\n",
        "    # Combine tokens from premise and hypothesis into one string per sample for both train and dev sets\n",
        "    train_set['combined_text'] = train_set.apply(\n",
        "    lambda row: ' '.join(row['premise_tokens'] + row['hypothesis_tokens']), axis=1)\n",
        "    dev_set['combined_text'] = dev_set.apply(\n",
        "    lambda row: ' '.join(row['premise_tokens'] + row['hypothesis_tokens']), axis=1)\n",
        "    # Combine both sets into one list of sentences\n",
        "    combined_sentences = pd.concat([train_set['combined_text'], dev_set['combined_text']])\n",
        "\n",
        "    # Save the combined sentences to a file for SentencePiece training\n",
        "    combined_sentences.to_csv(\"combined_train_dev.txt\", index=False, header=False)\n",
        "    # Train on your combined file\n",
        "    tokenizer.train(files=\"combined_train_dev.txt\", vocab_size=10000, min_frequency=2)\n",
        "    # Save the tokenizer model\n",
        "    tokenizer.save_model(\"byte_bpe\")\n",
        "\n",
        "#tokenizer = ByteLevelBPETokenizer(\"byte_bpe/vocab.json\", \"byte_bpe/merges.txt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqID3jQYhOZv"
      },
      "source": [
        "# TF-IDF word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMy6QCa2hOZv"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert tokenized text back to strings for TF-IDF processing\n",
        "train_premise_texts = train_set['premise_tokens'].apply(lambda x: ' '.join(x))\n",
        "train_hypothesis_texts = train_set['hypothesis_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "dev_premise_texts = dev_set['premise_tokens'].apply(lambda x: ' '.join(x))\n",
        "dev_hypothesis_texts = dev_set['hypothesis_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Combine premises & hypotheses for TF-IDF training\n",
        "all_texts = pd.concat([train_premise_texts, train_hypothesis_texts])\n",
        "\n",
        "# Initialize and fit TF-IDF on training data\n",
        "tfidf = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
        "tfidf.fit(all_texts)\n",
        "\n",
        "# Transform training and dev sets\n",
        "train_premise_tfidf = tfidf.transform(train_premise_texts).toarray()\n",
        "train_hypothesis_tfidf = tfidf.transform(train_hypothesis_texts).toarray()\n",
        "\n",
        "dev_premise_tfidf = tfidf.transform(dev_premise_texts).toarray()\n",
        "dev_hypothesis_tfidf = tfidf.transform(dev_hypothesis_texts).toarray()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJR8Bl-1fk1"
      },
      "source": [
        "# Glove embeddings and tfif wieghted word emeddings plus named eneitiy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTLuOgi81fk1"
      },
      "outputs": [],
      "source": [
        "glove = \"./glove_embeddings/glove.6B.100d.txt\"\n",
        "def load_glove(glove_file):\n",
        "    embeddings_dict = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype=np.float32)  # <-- Convert to float32\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "embedding_dim = 100\n",
        "loaded_glove = load_glove(glove)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l1pjn5z1fk1"
      },
      "outputs": [],
      "source": [
        "def sentence_embedding(tokens, embeddings_dict, embedding_dim):\n",
        "    valid_embeddings = [embeddings_dict[token] for token in tokens if token in embeddings_dict]\n",
        "    if not valid_embeddings:\n",
        "        # Return zero-vector if no embeddings found\n",
        "        return np.zeros(embedding_dim)\n",
        "    sentence_emb = np.mean(valid_embeddings, axis=0)\n",
        "    return sentence_emb\n",
        "\n",
        "def pairwise_embedding(premise_tokens, hypothesis_tokens, premise_tfidf, hypothesis_tfidf,  embeddings_dict,embedding_dim):\n",
        "    premise_emb = sentence_embedding(premise_tokens, embeddings_dict,embedding_dim)\n",
        "    hypothesis_emb = sentence_embedding(hypothesis_tokens, embeddings_dict,embedding_dim)\n",
        "    # Concatenate multiple useful features\n",
        "    combined_emb = np.concatenate([\n",
        "        premise_emb,\n",
        "        hypothesis_emb,\n",
        "        np.abs(premise_emb - hypothesis_emb), # capture difference\n",
        "        premise_emb * hypothesis_emb           # capture interactions\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "        # Concatenate TF-IDF features\n",
        "    combined_emb = np.concatenate([combined_emb, premise_tfidf, hypothesis_tfidf]).astype(np.float32)\n",
        "\n",
        "    return combined_emb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9GWj68p1fk2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Convert list of numpy arrays into a single 2D numpy array\n",
        "train_embeddings = np.stack(train_set.apply(\n",
        "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], train_premise_tfidf[x.name] ,train_hypothesis_tfidf[x.name] , loaded_glove, embedding_dim=100), axis=1))\n",
        "\n",
        "dev_embeddings = np.stack(dev_set.apply(\n",
        "    lambda x: pairwise_embedding(x['premise_tokens'], x['hypothesis_tokens'], dev_premise_tfidf[x.name] ,dev_hypothesis_tfidf[x.name], loaded_glove, embedding_dim=100), axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0alJR9Dd1fk2"
      },
      "source": [
        "# Traditional Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCXE9XvChOZw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puV0EXHm1fk2",
        "outputId": "6983d8a2-0683-4b6a-ee66-09aa4aca9bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 0.2],  # Regularization strength\n",
        "    'solver': ['lbfgs',],  # Different solvers for logistic regression\n",
        "    'max_iter': [500, 1000]  # More iterations for convergence\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
        "#clf = (solver='lbfgs', 'C'= 0.01, max_iter=500, cv=3, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
        "clf.fit(train_embeddings, train_set[\"label\"].values)  # Train on enhanced embeddings\n",
        "\n",
        "# Print Best Parameters\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "\n",
        "# Evaluate on validation set\n",
        "preds = clf.best_estimator_.predict(dev_embeddings)\n",
        "print(classification_report(dev_set['label'].values, preds, target_names=['entailment', 'contradiction']))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}